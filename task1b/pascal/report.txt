Team Naiveoutliers, Task1b, Pascal MÃ¼ller, 01.04.2021

APPROACH:  In a first step, the input vector is transformed from dimension 5 to 21, according to the feature order given in the task description. In a first try, a linear regression was implemented. The resulting weights were very large with a large variance. Hence, a ridge regression with inbuilt cross-validation was implemented to penalise growing weights. In order to find the suitable regularization parameter, a list of possible parameters was given as input. It was found that a 5-fold CV results in the best fit. Out of curiosity, an ElasticNet and a LassoCV model was also implemented in order to observe differences in these models. For all models, 'fit_intercept' was set to 'false', since after feature transformation of the inputs, the transformed matrix has already a column with ones (the constant transformation). It was found that the ridge regression delivered the best results regarding the RMSE, where the model complexity can be reduced without the loss of any variable, i.e. by setting a specific variable to zero weight.   CODE:  The code assumes that the 'train.csv' data file is stored in a folder called 'handout' which is located in the same directory as the folder with the script (line 22). E.g. Directory 'task1b' with subdirectory 'handout' where data is stored and subdirectory 'code' where the script is stored. The feature transformation is done using numpy and stacking the features horizontally (line 27). For the sake of simplicity and readability, the code parts for the other models are commented for (line 11-14, line 18-19, line 33-42, line 46-49).

